{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab7d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in and transforming data\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader,ConcatDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#from skimage import io, transform\n",
    "from PIL import Image\n",
    "\n",
    "# visualizing data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# load dataset information\n",
    "import yaml\n",
    "\n",
    "# image writing\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e19c5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "# WholeDatasetName = 'CVC-ClinicDB'\n",
    "WholeDatasetName = 'UTTQ'\n",
    "\n",
    "model_type = 'B0'\n",
    "_model_name = 'ESFP_{}_Endo_{}'.format(model_type,WholeDatasetName)\n",
    "config = open('Configure.yaml')\n",
    "config = yaml.safe_load(config)\n",
    "\n",
    "init_trainsize = 312\n",
    "batch_size = 8\n",
    "\n",
    "repeats = 1\n",
    "n_epochs = 100\n",
    "if_renew = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7b2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplittingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    dataloader for polyp segmentation tasks\n",
    "    \"\"\"\n",
    "    def __init__(self, image_root, gt_root):\n",
    "\n",
    "        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpeg') or f.endswith('.png') or f.endswith('.jpg')]\n",
    "        # print(len(self.images))\n",
    "        self.gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.png') or f.endswith('.jpeg') or f.endswith('.jpg')]\n",
    "        # print(len(self.gts))\n",
    "        self.images = sorted(self.images)\n",
    "        self.gts = sorted(self.gts)\n",
    "        self.filter_files()\n",
    "        self.size = len(self.images)\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.rgb_loader(self.images[index])\n",
    "        gt = self.binary_loader(self.gts[index])\n",
    "        name = self.images[index].split('/')[-1]\n",
    "        return self.transform(image), self.transform(gt), name\n",
    "\n",
    "    def filter_files(self):\n",
    "        assert len(self.images) == len(self.gts)\n",
    "        images = []\n",
    "        gts = []\n",
    "        for img_path, gt_path in zip(self.images, self.gts):\n",
    "            img = Image.open(img_path)\n",
    "            gt = Image.open(gt_path)\n",
    "            if img.size == gt.size:\n",
    "                images.append(img_path)\n",
    "                gts.append(gt_path)\n",
    "        self.images = images\n",
    "        self.gts = gts\n",
    "\n",
    "    def rgb_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "\n",
    "    def binary_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('L')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9aa2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(renew, root_dir):\n",
    "    \n",
    "    split_train_images_save_path = './Endoscope-WL/{}_Splited/trainSplited/images'.format(WholeDatasetName)\n",
    "    os.makedirs(split_train_images_save_path, exist_ok=True)\n",
    "    split_train_masks_save_path = './Endoscope-WL/{}_Splited/trainSplited/masks'.format(WholeDatasetName)\n",
    "    os.makedirs(split_train_masks_save_path, exist_ok=True)\n",
    "    \n",
    "    split_validation_images_save_path = './Endoscope-WL/{}_Splited/validationSplited/images'.format(WholeDatasetName)\n",
    "    os.makedirs(split_validation_images_save_path, exist_ok=True)\n",
    "    split_validation_masks_save_path = './Endoscope-WL/{}_Splited/validationSplited/masks'.format(WholeDatasetName)\n",
    "    os.makedirs(split_validation_masks_save_path, exist_ok=True)\n",
    "    \n",
    "    split_test_images_save_path = './Endoscope-WL/{}_Splited/testSplited/images'.format(WholeDatasetName)\n",
    "    os.makedirs(split_test_images_save_path, exist_ok=True)\n",
    "    split_test_masks_save_path = './Endoscope-WL/{}_Splited/testSplited/masks'.format(WholeDatasetName)\n",
    "    os.makedirs(split_test_masks_save_path, exist_ok=True)\n",
    "    \n",
    "    if renew == True:\n",
    "    \n",
    "        DatasetList = []\n",
    "        \n",
    "        # images_train_path = config['dataset']['train_' + str(WholeDatasetName) + '_dataset'] + '/images/'\n",
    "        images_train_path = root_dir + \"/train/images/\"\n",
    "        # masks_train_path = config['dataset']['train_' + str(WholeDatasetName) + '_dataset'] + '/masks/'\n",
    "        masks_train_path = root_dir + \"/train/mask_images/\"\n",
    "        Dataset_part_train = SplittingDataset(images_train_path, masks_train_path)\n",
    "        DatasetList.append(Dataset_part_train)\n",
    "         # images_test_path = config['dataset']['test_' + str(WholeDatasetName) + '_img']\n",
    "        images_val_path = root_dir + \"/valid/images/\"\n",
    "        # # masks_test_path = config['dataset']['test_' + str(WholeDatasetName) + '_label']\n",
    "        masks_val_path = root_dir + \"/valid/mask_images/\"\n",
    "        Dataset_part_val = SplittingDataset(images_val_path, masks_val_path)\n",
    "        DatasetList.append(Dataset_part_val)\n",
    "\n",
    "        # images_test_path = config['dataset']['test_' + str(WholeDatasetName) + '_img']\n",
    "        images_test_path = root_dir + \"/test/images/\"\n",
    "        # # masks_test_path = config['dataset']['test_' + str(WholeDatasetName) + '_label']\n",
    "        masks_test_path = root_dir + \"/test/mask_images/\"\n",
    "        Dataset_part_test = SplittingDataset(images_test_path, masks_test_path)\n",
    "        DatasetList.append(Dataset_part_test)\n",
    "                                    \n",
    "        wholeDataset = ConcatDataset([DatasetList[0], DatasetList[1], DatasetList[2]])\n",
    "        # val_num = int(0.1*len(wholeDataset))\n",
    "        # test_num = int(0.1*len(wholeDataset))\n",
    "                                    \n",
    "        # train_num = len(wholeDataset) - val_num - test_num\n",
    "        val_num = len(os.listdir(root_dir + \"/valid/images/\"))\n",
    "        # print(val_num)\n",
    "        # test_num = len(os.listdir(root_dir + \"/new_test/images/\"))\n",
    "        train_num = len(os.listdir(root_dir + \"/train/images/\"))\n",
    "        # print(train_num)\n",
    "        \n",
    "        # train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(wholeDataset, [train_num, val_num, test_num])\n",
    "        train_dataset = torch.utils.data.Subset(wholeDataset, range(train_num))\n",
    "        val_dataset = torch.utils.data.Subset(wholeDataset, range(train_num, train_num + val_num))\n",
    "        test_dataset = torch.utils.data.Subset(wholeDataset, range(train_num + val_num, len(wholeDataset)))\n",
    "                           \n",
    "        train_loader = DataLoader(dataset=train_dataset,batch_size=1,shuffle=False)\n",
    "        val_loader = DataLoader(dataset=val_dataset,batch_size=1,shuffle=False)\n",
    "        test_loader = DataLoader(dataset=test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "        iter_train = iter(train_loader)\n",
    "        for i in range(len(train_loader)):\n",
    "            image, gt, name = iter_train.next()\n",
    "            image_data = image.data.cpu().numpy().squeeze().transpose(1,2,0)\n",
    "            gt_data = gt.data.cpu().numpy().squeeze()\n",
    "            imageio.imwrite(split_train_images_save_path + '/' + name[0],img_as_ubyte(image_data))\n",
    "            imageio.imwrite(split_train_masks_save_path + '/' + name[0],img_as_ubyte(gt_data))\n",
    "            \n",
    "        iter_val = iter(val_loader)\n",
    "        for i in range(len(val_loader)):\n",
    "            image, gt, name = iter_val.next()\n",
    "            image_data = image.data.cpu().numpy().squeeze().transpose(1,2,0)\n",
    "            gt_data = gt.data.cpu().numpy().squeeze()\n",
    "            imageio.imwrite(split_validation_images_save_path + '/' + name[0],img_as_ubyte(image_data))\n",
    "            imageio.imwrite(split_validation_masks_save_path + '/' + name[0],img_as_ubyte(gt_data))\n",
    "            \n",
    "        \n",
    "        iter_test = iter(test_loader)\n",
    "        for i in range(len(test_loader)):\n",
    "            image, gt, name = iter_test.next()\n",
    "            image_data = image.data.cpu().numpy().squeeze().transpose(1,2,0)\n",
    "            gt_data = gt.data.cpu().numpy().squeeze()\n",
    "            imageio.imwrite(split_test_images_save_path + '/' + name[0],img_as_ubyte(image_data))\n",
    "            imageio.imwrite(split_test_masks_save_path + '/' + name[0],img_as_ubyte(gt_data))        \n",
    "    \n",
    "    return split_train_images_save_path, split_train_masks_save_path, split_validation_images_save_path, split_validation_masks_save_path, split_test_images_save_path, split_test_masks_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60f6ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_renew = False\n",
    "root_dir = \"/home/baoanh/baoanh/DATN/Ungthuthucquan\"\n",
    "train_images_path, train_masks_path, val_images_path, val_masks_path, test_images_path, test_masks_path = splitDataset(if_renew, root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9276126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = \"/home/baoanh/baoanh/DATN/dataset/Viem_daday_hp_am\"\n",
    "# train_images_path = root_dir + \"/new_train/images\"\n",
    "# train_masks_path = root_dir + \"/new_train/mask_images\"\n",
    "# val_images_path = root_dir + \"/new_valid/images\"\n",
    "# val_masks_path = root_dir + \"/new_valid/mask_images\"\n",
    "# test_images_path = root_dir + \"/new_test/images\"\n",
    "# test_masks_path = root_dir + \"/new_test/mask_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e5a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolypDataset(Dataset):\n",
    "    \"\"\"\n",
    "    dataloader for polyp segmentation tasks\n",
    "    \"\"\"\n",
    "    def __init__(self, image_root, gt_root, trainsize, augmentations):\n",
    "        self.trainsize = trainsize\n",
    "        self.augmentations = augmentations\n",
    "        # print(self.augmentations)\n",
    "        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpeg') or f.endswith('.png') or f.endswith('.jpg')]\n",
    "        #print(image_root)\n",
    "        self.gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.png') or f.endswith('.jpeg') or f.endswith('.jpg')]\n",
    "        self.images = sorted(self.images)\n",
    "        self.gts = sorted(self.gts)\n",
    "        self.filter_files()\n",
    "        self.size = len(self.images)\n",
    "        if self.augmentations == True:\n",
    "            print('Using RandomRotation, RandomFlip')\n",
    "            self.img_transform = transforms.Compose([\n",
    "                transforms.RandomRotation(90, resample=False, expand=False, center=None),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.Resize((self.trainsize, self.trainsize)),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0, hue=0),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])])\n",
    "            self.gt_transform = transforms.Compose([\n",
    "                transforms.RandomRotation(90, resample=False, expand=False, center=None),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.Resize((self.trainsize, self.trainsize)),\n",
    "                transforms.ToTensor()])\n",
    "            \n",
    "        else:\n",
    "            print('no augmentation')\n",
    "            self.img_transform = transforms.Compose([\n",
    "                transforms.Resize((self.trainsize, self.trainsize)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])])\n",
    "            \n",
    "            self.gt_transform = transforms.Compose([\n",
    "                transforms.Resize((self.trainsize, self.trainsize)),\n",
    "                transforms.ToTensor()])\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.rgb_loader(self.images[index])\n",
    "        gt = self.binary_loader(self.gts[index])\n",
    "        \n",
    "        seed = np.random.randint(2147483647) # make a seed with numpy generator \n",
    "        np.random.seed(seed) # apply this seed to img tranfsorms\n",
    "        torch.manual_seed(seed) # needed for torchvision 0.7\n",
    "        if self.img_transform is not None:\n",
    "            image = self.img_transform(image)\n",
    "            \n",
    "        np.random.seed(seed) # apply this seed to img tranfsorms\n",
    "        torch.manual_seed(seed) # needed for torchvision 0.7\n",
    "        if self.gt_transform is not None:\n",
    "            gt = self.gt_transform(gt)\n",
    "        return image, gt\n",
    "\n",
    "    def filter_files(self):\n",
    "        assert len(self.images) == len(self.gts)\n",
    "        images = []\n",
    "        gts = []\n",
    "        for img_path, gt_path in zip(self.images, self.gts):\n",
    "            img = Image.open(img_path)\n",
    "            gt = Image.open(gt_path)\n",
    "            if img.size == gt.size:\n",
    "                images.append(img_path)\n",
    "                gts.append(gt_path)\n",
    "        self.images = images\n",
    "        self.gts = gts\n",
    "\n",
    "    def rgb_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "\n",
    "    def binary_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            # return img.convert('1')\n",
    "            return img.convert('L')\n",
    "\n",
    "    def resize(self, img, gt):\n",
    "        assert img.size == gt.size\n",
    "        w, h = img.size\n",
    "        if h < self.trainsize or w < self.trainsize:\n",
    "            h = max(h, self.trainsize)\n",
    "            w = max(w, self.trainsize)\n",
    "            return img.resize((w, h), Image.BILINEAR), gt.resize((w, h), Image.NEAREST)\n",
    "        else:\n",
    "            return img, gt\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "class test_dataset:\n",
    "    def __init__(self, image_root, gt_root, testsize):\n",
    "        self.testsize = testsize\n",
    "        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpeg') or f.endswith('.png') or f.endswith('.jpg')]\n",
    "        self.gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.tif') or f.endswith('.png') or f.endswith('.jpeg') or f.endswith('.jpg')]\n",
    "        self.images = sorted(self.images)\n",
    "        self.gts = sorted(self.gts)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((self.testsize, self.testsize)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                 [0.229, 0.224, 0.225])])\n",
    "        self.gt_transform = transforms.ToTensor()\n",
    "        self.size = len(self.images)\n",
    "        self.index = 0\n",
    "\n",
    "    def load_data(self):\n",
    "        image = self.rgb_loader(self.images[self.index])\n",
    "        image = self.transform(image).unsqueeze(0)\n",
    "        gt = self.binary_loader(self.gts[self.index])\n",
    "        name = self.images[self.index].split('/')[-1]\n",
    "        if name.endswith('.jpeg'):\n",
    "            name = name.split('.jpeg')[0] + '.png'\n",
    "        self.index += 1\n",
    "        return image, gt, name\n",
    "\n",
    "    def rgb_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "\n",
    "    def binary_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc993709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoder import mit\n",
    "from Decoder import mlp\n",
    "from mmcv.cnn import ConvModule\n",
    "\n",
    "class ESFPNetStructure(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim = 160):\n",
    "        super(ESFPNetStructure, self).__init__()\n",
    "        \n",
    "        # Backbone\n",
    "        if model_type == 'B0':\n",
    "            self.backbone = mit.mit_b0()\n",
    "        if model_type == 'B1':\n",
    "            self.backbone = mit.mit_b1()\n",
    "        if model_type == 'B2':\n",
    "            self.backbone = mit.mit_b2()\n",
    "        if model_type == 'B3':\n",
    "            self.backbone = mit.mit_b3()\n",
    "        if model_type == 'B4':\n",
    "            self.backbone = mit.mit_b4()\n",
    "        if model_type == 'B5':\n",
    "            self.backbone = mit.mit_b5()\n",
    "        \n",
    "        self._init_weights()  # load pretrain\n",
    "        \n",
    "        # LP Header\n",
    "        self.LP_1 = mlp.LP(input_dim = self.backbone.embed_dims[0], embed_dim = self.backbone.embed_dims[0])\n",
    "        self.LP_2 = mlp.LP(input_dim = self.backbone.embed_dims[1], embed_dim = self.backbone.embed_dims[1])\n",
    "        self.LP_3 = mlp.LP(input_dim = self.backbone.embed_dims[2], embed_dim = self.backbone.embed_dims[2])\n",
    "        self.LP_4 = mlp.LP(input_dim = self.backbone.embed_dims[3], embed_dim = self.backbone.embed_dims[3])\n",
    "        \n",
    "        # Linear Fuse\n",
    "        self.linear_fuse34 = ConvModule(in_channels=(self.backbone.embed_dims[2] + self.backbone.embed_dims[3]), out_channels=self.backbone.embed_dims[2], kernel_size=1,norm_cfg=dict(type='BN', requires_grad=True))\n",
    "        self.linear_fuse23 = ConvModule(in_channels=(self.backbone.embed_dims[1] + self.backbone.embed_dims[2]), out_channels=self.backbone.embed_dims[1], kernel_size=1,norm_cfg=dict(type='BN', requires_grad=True))\n",
    "        self.linear_fuse12 = ConvModule(in_channels=(self.backbone.embed_dims[0] + self.backbone.embed_dims[1]), out_channels=self.backbone.embed_dims[0], kernel_size=1,norm_cfg=dict(type='BN', requires_grad=True))\n",
    "        \n",
    "        # Fused LP Header\n",
    "        self.LP_12 = mlp.LP(input_dim = self.backbone.embed_dims[0], embed_dim = self.backbone.embed_dims[0])\n",
    "        self.LP_23 = mlp.LP(input_dim = self.backbone.embed_dims[1], embed_dim = self.backbone.embed_dims[1])\n",
    "        self.LP_34 = mlp.LP(input_dim = self.backbone.embed_dims[2], embed_dim = self.backbone.embed_dims[2])\n",
    "        \n",
    "        # Final Linear Prediction\n",
    "        self.linear_pred = nn.Conv2d((self.backbone.embed_dims[0] + self.backbone.embed_dims[1] + self.backbone.embed_dims[2] + self.backbone.embed_dims[3]), 1, kernel_size=1)\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \n",
    "        if model_type == 'B0':\n",
    "            pretrained_dict = torch.load('./Pretrained/mit_b0.pth')\n",
    "        if model_type == 'B1':\n",
    "            pretrained_dict = torch.load('./Pretrained/mit_b1.pth')\n",
    "        if model_type == 'B2':\n",
    "            pretrained_dict = torch.load('./Pretrained/mit_b2.pth')\n",
    "        if model_type == 'B3':\n",
    "            pretrained_dict = torch.load('./Pretrained/mit_b3.pth')\n",
    "        if model_type == 'B4':\n",
    "            pretrained_dict = torch.load('./Pretrained/mit_b4.pth')\n",
    "        if model_type == 'B5':\n",
    "            pretrained_dict = torch.load('./Pretrained/mit_b5.pth')\n",
    "            \n",
    "            \n",
    "        model_dict = self.backbone.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.backbone.load_state_dict(model_dict)\n",
    "        print(\"successfully loaded!!!!\")\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ##################  Go through backbone ###################\n",
    "        \n",
    "        B = x.shape[0]\n",
    "        \n",
    "        #stage 1\n",
    "        out_1, H, W = self.backbone.patch_embed1(x)\n",
    "        for i, blk in enumerate(self.backbone.block1):\n",
    "            out_1 = blk(out_1, H, W)\n",
    "        out_1 = self.backbone.norm1(out_1)\n",
    "        out_1 = out_1.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()  #(Batch_Size, self.backbone.embed_dims[0], 88, 88)\n",
    "        \n",
    "        # stage 2\n",
    "        out_2, H, W = self.backbone.patch_embed2(out_1)\n",
    "        for i, blk in enumerate(self.backbone.block2):\n",
    "            out_2 = blk(out_2, H, W)\n",
    "        out_2 = self.backbone.norm2(out_2)\n",
    "        out_2 = out_2.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()  #(Batch_Size, self.backbone.embed_dims[1], 44, 44)\n",
    "        \n",
    "        # stage 3\n",
    "        out_3, H, W = self.backbone.patch_embed3(out_2)\n",
    "        for i, blk in enumerate(self.backbone.block3):\n",
    "            out_3 = blk(out_3, H, W)\n",
    "        out_3 = self.backbone.norm3(out_3)\n",
    "        out_3 = out_3.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()  #(Batch_Size, self.backbone.embed_dims[2], 22, 22)\n",
    "        \n",
    "        # stage 4\n",
    "        out_4, H, W = self.backbone.patch_embed4(out_3)\n",
    "        for i, blk in enumerate(self.backbone.block4):\n",
    "            out_4 = blk(out_4, H, W)\n",
    "        out_4 = self.backbone.norm4(out_4)\n",
    "        out_4 = out_4.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()  #(Batch_Size, self.backbone.embed_dims[3], 11, 11)\n",
    "        \n",
    "        # go through LP Header\n",
    "        lp_1 = self.LP_1(out_1)\n",
    "        lp_2 = self.LP_2(out_2)  \n",
    "        lp_3 = self.LP_3(out_3)  \n",
    "        lp_4 = self.LP_4(out_4)\n",
    "        \n",
    "        # linear fuse and go pass LP Header\n",
    "        lp_34 = self.LP_34(self.linear_fuse34(torch.cat([lp_3, F.interpolate(lp_4,scale_factor=2,mode='bilinear', align_corners=False)], dim=1)))\n",
    "        lp_23 = self.LP_23(self.linear_fuse23(torch.cat([lp_2, F.interpolate(lp_34,scale_factor=2,mode='bilinear', align_corners=False)], dim=1)))\n",
    "        lp_12 = self.LP_12(self.linear_fuse12(torch.cat([lp_1, F.interpolate(lp_23,scale_factor=2,mode='bilinear', align_corners=False)], dim=1)))\n",
    "        \n",
    "        # get the final output\n",
    "        lp4_resized = F.interpolate(lp_4,scale_factor=8,mode='bilinear', align_corners=False)\n",
    "        lp3_resized = F.interpolate(lp_34,scale_factor=4,mode='bilinear', align_corners=False)\n",
    "        lp2_resized = F.interpolate(lp_23,scale_factor=2,mode='bilinear', align_corners=False)\n",
    "        lp1_resized = lp_12\n",
    "        \n",
    "        out = self.linear_pred(torch.cat([lp1_resized, lp2_resized, lp3_resized, lp4_resized], dim=1))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e858b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "\n",
    "# def calc_dist_map(seg):\n",
    "#     res = np.zeros_like(seg)\n",
    "#     posmask = seg.astype(np.bool)\n",
    "\n",
    "#     if posmask.any():\n",
    "#         negmask = ~posmask\n",
    "#         res = distance_transform_edt(negmask) * negmask - (distance_transform_edt(posmask) - 1) * posmask\n",
    "\n",
    "#     return res.astype(np.float32)\n",
    "\n",
    "# def calc_dist_map_batch(y_true):\n",
    "#     y_true_numpy = y_true.cpu().detach().numpy()\n",
    "#     return np.array([calc_dist_map(y) for y in y_true_numpy])\n",
    "\n",
    "# def surface_loss_torch(y_pred, y_true):\n",
    "#     import torch\n",
    "#     import torch.nn.functional as F\n",
    "#     y_true_dist_map = torch.from_numpy(calc_dist_map_batch(y_true)).to(y_pred.device)\n",
    "#     multipled = y_pred * y_true_dist_map\n",
    "#     return torch.mean(multipled)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def focal_loss(pred, target, gamma=2, alpha=0.25, smooth=0.0001):\n",
    "    num = target.size(0)\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    dice_coefficient = (2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)\n",
    "\n",
    "    # Calculate the focal weights for each pixel\n",
    "    focal_weights = torch.pow(1 - dice_coefficient, gamma)\n",
    "\n",
    "    # Calculate the focal loss (element-wise multiplication with alpha)\n",
    "    focal_loss = -alpha * torch.log(dice_coefficient) * focal_weights\n",
    "\n",
    "    return focal_loss.sum() / num\n",
    "def ange_structure_loss(pred, mask, smooth=1):\n",
    "    \n",
    "    weit = 1 + 5*torch.abs(F.avg_pool2d(mask, kernel_size=15, stride=1, padding=7) - mask)\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduction='mean')\n",
    "    wbce = (weit*wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask)*weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask)*weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + smooth)/(union - inter + smooth)\n",
    "    wfocal = focal_loss(pred, mask) \n",
    "    wfocal = (weit*wfocal).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "    return (wbce + wiou + wfocal).mean()\n",
    "\n",
    "def dice_loss_coff(pred, target, smooth = 0.0001):\n",
    "    \n",
    "    num = target.size(0)\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)\n",
    "    \n",
    "    return loss.sum()/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c112d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def evaluate():  \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ESFPNet.eval()\n",
    "    \n",
    "    val = 0\n",
    "    count = 0\n",
    "\n",
    "    smooth = 1e-4\n",
    "    \n",
    "    val_loader = test_dataset(val_images_path + '/',val_masks_path + '/', init_trainsize)\n",
    "    for i in range(val_loader.size):\n",
    "        image, gt, name = val_loader.load_data()\n",
    "        gt = np.asarray(gt, np.float32)\n",
    "        gt /= (gt.max() + 1e-8)\n",
    "\n",
    "        image = image.cuda()\n",
    "        \n",
    "        pred= ESFPNet(image)\n",
    "        pred = F.upsample(pred, size=gt.shape, mode='bilinear', align_corners=False)\n",
    "        pred = pred.sigmoid()\n",
    "        threshold = torch.tensor([0.5]).to(device)\n",
    "        pred = (pred > threshold).float() * 1\n",
    "\n",
    "        pred = pred.data.cpu().numpy().squeeze()\n",
    "        pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "        \n",
    "        target = np.array(gt)\n",
    "        \n",
    "        input_flat = np.reshape(pred,(-1))\n",
    "        target_flat = np.reshape(target,(-1))\n",
    " \n",
    "        intersection = (input_flat*target_flat)\n",
    "        \n",
    "        loss =  (2 * intersection.sum() + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "\n",
    "        a =  '{:.4f}'.format(loss)\n",
    "        a = float(a)\n",
    "        \n",
    "        val = val + a\n",
    "        count = count + 1\n",
    "        \n",
    "    ESFPNet.train()\n",
    "    \n",
    "    return val/count, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ce57ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "def load_model(model, model_path):\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f\"Loaded pre-trained model from {model_path}\")\n",
    "    else:\n",
    "        print(\"No pre-trained model found. Starting from scratch.\")\n",
    "def training_loop(n_epochs, model, ESFPNet_optimizer, numIters):\n",
    "    \n",
    "    # keep track of losses over time\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    coeff_max = 0\n",
    "    \n",
    "    # set up data and then train\n",
    "    trainDataset = PolypDataset(train_images_path + '/', train_masks_path + '/', trainsize=init_trainsize, augmentations = True)\n",
    "    train_loader = DataLoader(dataset=trainDataset,batch_size=batch_size,shuffle=True)\n",
    "    \n",
    "    iter_X = iter(train_loader)\n",
    "    steps_per_epoch = len(iter_X)\n",
    "    num_epoch = 0\n",
    "    total_steps = (n_epochs+1)*steps_per_epoch\n",
    "    num_to_stop = 0\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for step in range(1, total_steps):\n",
    "\n",
    "        # Reset iterators for each epoch\n",
    "        if step % steps_per_epoch == 0:\n",
    "            iter_X = iter(train_loader)\n",
    "            num_epoch = num_epoch + 1\n",
    "        \n",
    "        # make sure to scale to a range -1 to 1\n",
    "        images, masks = iter_X.next()\n",
    "        \n",
    "        # move images to GPU if available (otherwise stay on CPU)\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "\n",
    "        # ============================================\n",
    "        #            TRAIN THE NETWORKS\n",
    "        # ============================================\n",
    "       \n",
    "        ESFPNet_optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Compute the losses from the network\n",
    "        \n",
    "        out = ESFPNet(images)\n",
    "        out = F.interpolate(out, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        loss = ange_structure_loss(out, masks)\n",
    "        # boundary_loss = DistBinaryDiceLoss()\n",
    "        # loss = boundary_loss(out, masks)\n",
    "        # print(out.shape, masks.shape)\n",
    "        # l1_reg = torch.tensor(0.0).to(device)\n",
    "        l2_reg = torch.tensor(0.0).to(device)\n",
    "        for module in model.modules():\n",
    "            mask = None\n",
    "            weight = None\n",
    "            for name, buffer in module.named_buffers():\n",
    "                if name == \"weight_mask\":\n",
    "                    mask = buffer\n",
    "            for name, param in module.named_parameters():\n",
    "                if name == \"weight_orig\":\n",
    "                    weight = param\n",
    "            if mask is not None and weight is not None:\n",
    "                # l1_reg += torch.norm(mask * weight, 1)\n",
    "                l2_reg += 0.5 * torch.norm(mask * weight, 2) \n",
    "        l1_regularization_strength = 1e-3\n",
    "        loss += l1_regularization_strength * l2_reg\n",
    "        loss.backward()\n",
    "        ESFPNet_optimizer.step() \n",
    "        \n",
    "        # ============================================\n",
    "        #            TRAIN THE NETWORKS\n",
    "        # ============================================\n",
    "        # Print the log info\n",
    "        if step % steps_per_epoch == 0:\n",
    "            save_model_path = './SaveModel/{}_LA_{:1d}'.format(_model_name,numIters)\n",
    "            os.makedirs(save_model_path, exist_ok=True)\n",
    "            losses.append(loss.item())\n",
    "            print('Epoch [{:5d}/{:5d}] | preliminary loss: {:6.6f} '.format(num_epoch, n_epochs, loss.item()))\n",
    "            # torch.save(ESFPNet, save_model_path + f'/ESFPNet_{num_epoch}.pt')\n",
    "        if step % steps_per_epoch == 0:\n",
    "            \n",
    "            validation_coeff, val_loss = evaluate()\n",
    "            val_losses.append(val_loss.item())\n",
    "            print('Epoch [{:5d}/{:5d}] | validation_coeffient: {:6.6f} '.format(\n",
    "                    num_epoch, n_epochs, validation_coeff))\n",
    "            \n",
    "            if coeff_max < validation_coeff:\n",
    "                coeff_max = validation_coeff\n",
    "                save_model_path = './SaveModel/{}_LA_{:1d}'.format(_model_name,numIters)\n",
    "                os.makedirs(save_model_path, exist_ok=True)\n",
    "                # print(save_model_path)\n",
    "                torch.save(ESFPNet, save_model_path + '/ESFPNet.pt')\n",
    "                print('Save Learning Ability Optimized Model at Epoch [{:5d}/{:5d}]'.format(num_epoch, n_epochs))\n",
    "                num_to_stop = 0\n",
    "            else:\n",
    "                num_to_stop = num_to_stop + 1\n",
    "    \n",
    "        if num_to_stop >=10:\n",
    "            break\n",
    "                \n",
    "    return losses,val_losses, coeff_max, num_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d06570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResult(numIters):\n",
    "    \n",
    "    save_path = './results/{}_LA_{:1d}/{}_Splited/'.format(_model_name,numIters,str(WholeDatasetName))\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    print(save_path)\n",
    "        \n",
    "    model_path =  './SaveModel/{}_LA_{:1d}'.format(_model_name,numIters)\n",
    "    ESFPNetBest = torch.load(model_path + '/ESFPNet.pt')\n",
    "    ESFPNetBest.eval()\n",
    "    \n",
    "    test_loader = test_dataset(test_images_path + '/', test_masks_path + '/', init_trainsize)\n",
    "    for i in range(test_loader.size):\n",
    "        image, gt, name = test_loader.load_data()\n",
    "        gt = np.asarray(gt, np.float32)\n",
    "        gt /= (gt.max() + 1e-8)\n",
    "        image = image.cuda()\n",
    "\n",
    "        pred = ESFPNetBest(image)\n",
    "        pred = F.upsample(pred, size=gt.shape, mode='bilinear', align_corners=False)\n",
    "        pred = pred.sigmoid()\n",
    "        threshold = torch.tensor([0.5]).to(device)\n",
    "        pred = (pred > threshold).float() * 1\n",
    "        pred = pred.data.cpu().numpy().squeeze()\n",
    "        pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "        \n",
    "\n",
    "        imageio.imwrite(save_path+name,img_as_ubyte(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114565ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded!!!!\n",
      "Models moved to GPU.\n",
      "#####################################################################################\n",
      "Using RandomRotation, RandomFlip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baoanh/miniconda3/envs/esfp/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1292: UserWarning: The parameter 'resample' is deprecated since 0.12 and will be removed 0.14. Please use 'interpolation' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 39 but got size 40 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m \u001b[39m#0.0001\u001b[39;00m\n\u001b[1;32m     20\u001b[0m ESFPNet_optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdamW(ESFPNet\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr)\n\u001b[0;32m---> 22\u001b[0m losses, val_losses, coeff_max, num_epoch \u001b[39m=\u001b[39m training_loop(n_epochs,ESFPNet, ESFPNet_optimizer, i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     23\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), losses, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), val_losses, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation Loss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 49\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, model, ESFPNet_optimizer, numIters)\u001b[0m\n\u001b[1;32m     45\u001b[0m ESFPNet_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     47\u001b[0m \u001b[39m# 1. Compute the losses from the network\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m out \u001b[39m=\u001b[39m ESFPNet(images)\n\u001b[1;32m     50\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(out, scale_factor\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m loss \u001b[39m=\u001b[39m ange_structure_loss(out, masks)\n",
      "File \u001b[0;32m~/miniconda3/envs/esfp/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 110\u001b[0m, in \u001b[0;36mESFPNetStructure.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m# linear fuse and go pass LP Header\u001b[39;00m\n\u001b[1;32m    109\u001b[0m lp_34 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLP_34(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_fuse34(torch\u001b[39m.\u001b[39mcat([lp_3, F\u001b[39m.\u001b[39minterpolate(lp_4,scale_factor\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[0;32m--> 110\u001b[0m lp_23 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLP_23(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_fuse23(torch\u001b[39m.\u001b[39;49mcat([lp_2, F\u001b[39m.\u001b[39;49minterpolate(lp_34,scale_factor\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbilinear\u001b[39;49m\u001b[39m'\u001b[39;49m, align_corners\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)))\n\u001b[1;32m    111\u001b[0m lp_12 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLP_12(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_fuse12(torch\u001b[39m.\u001b[39mcat([lp_1, F\u001b[39m.\u001b[39minterpolate(lp_23,scale_factor\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[1;32m    113\u001b[0m \u001b[39m# get the final output\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 39 but got size 40 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "for i in range(repeats):\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ESFPNet = ESFPNetStructure()\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        ESFPNet.to(device)\n",
    "        print('Models moved to GPU.')\n",
    "    else:\n",
    "        print('Only CPU available.')\n",
    "    print('#####################################################################################')  \n",
    "        \n",
    "    # hyperparams for Adam optimizer\n",
    "    lr=0.0001 #0.0001\n",
    "\n",
    "    ESFPNet_optimizer = optim.AdamW(ESFPNet.parameters(), lr=lr)\n",
    "\n",
    "    losses, val_losses, coeff_max, num_epoch = training_loop(n_epochs,ESFPNet, ESFPNet_optimizer, i+1)\n",
    "    plt.plot(range(1, num_epoch+1), losses, color='b', label='Training Loss')\n",
    "    plt.plot(range(1, num_epoch+1), val_losses, color='r', label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    print('#####################################################################################')  \n",
    "    print('optimize_m_dice: {:6.6f}'.format(coeff_max))\n",
    "    plt.savefig(f\"visualize_{str(WholeDatasetName)}.png\")\n",
    "    saveResult(i+1)\n",
    "    print('#####################################################################################')  \n",
    "    print('saved the results')\n",
    "    print('#####################################################################################')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34622ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 55\n",
      "0.8950418053826019 0.8100232296050968 0.9334296296787103 0.8596867009623541 0.9349839190841966\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn = sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = sum((y_true == 0) & (y_pred == 1))\n",
    "    specificity = tn / (tn + fp)\n",
    "    return  specificity\n",
    "# Folder paths for masks and predicted masks\n",
    "# Paths to folders containing masks and predicted images\n",
    "mask_folder = \"/home/baoanh/baoanh/DATN/dataset/Ungthuthucquan/test/mask_images/*.png\"\n",
    "predicted_mask_folder = \"/home/baoanh/baoanh/DATN/ESFPNet/results/ESFP_B0_Endo_UTTQ_LA_1/UTTQ_Splited/*.jpeg\"\n",
    "# mask_folder = f\"/home/baoanh/baoanh/DATN/ESFPNet/Endoscope-WL/{WholeDatasetName}_Splited/testSplited/masks/*.jpeg\"\n",
    "# mask_folder = \"/home/baoanh/baoanh/DATN/dataset/Viem_daday_hp_am/new_test/mask_images/*.png\"\n",
    "# predicted_mask_folder = f\"/home/baoanh/baoanh/DATN/ESFPNet/results/ESFP_B0_Endo_{WholeDatasetName}_LA_1/{WholeDatasetName}_Splited/*.png\"\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "mask_images = sorted(glob.glob(mask_folder))\n",
    "predicted_mask_images = sorted(glob.glob(predicted_mask_folder))\n",
    "print(len(mask_images), len(predicted_mask_images))\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import jaccard_score, f1_score, precision_score, recall_score\n",
    "for i in range(len(mask_images)):\n",
    "    pred = np.ndarray.flatten(imread(predicted_mask_images[i]) / 255) > 0.5\n",
    "    gt = np.ndarray.flatten(imread(mask_images[i])) \n",
    "    # print(imread(predicted_mask_images[i]).shape)\n",
    "    # print(imread(mask_images[i])[:,:,0].shape)\n",
    "    # print(np.unique(imread(mask_images[i], 0)))\n",
    "    # print(np.unique(imread(predicted_mask_images[i])))\n",
    "    dice = []\n",
    "    IoU = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    specificity = []\n",
    "    dice.append(f1_score(gt, pred))\n",
    "    IoU.append(jaccard_score(gt, pred))\n",
    "    precision.append(precision_score(gt, pred))\n",
    "    recall.append(recall_score(gt, pred))\n",
    "    specificity.append(specificity_score(gt, pred))\n",
    "    # break\n",
    "print(  np.mean(dice),\n",
    "        np.mean(IoU),\n",
    "        np.mean(precision),\n",
    "        np.mean(recall),\n",
    "        np.mean(specificity),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c193da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n",
      "[False  True]\n",
      "mDice: 0.8657421821943052\n",
      "mIoU: 0.8333686523165287\n",
      "Recall: 0.8295869252187139\n",
      "Precision: 0.9051924994991245\n",
      "mSpecificity: 0.9651761249274172\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from PIL import Image\n",
    "from sklearn.metrics import jaccard_score, f1_score, confusion_matrix\n",
    "from skimage import img_as_bool\n",
    "from PIL import Image\n",
    "# Paths to folders containing masks and predicted images\n",
    "mask_folder = \"/home/baoanh/baoanh/DATN/ESFPNet/Endoscope-WL/UTTQ_Splited/testSplited/masks\"\n",
    "predicted_folder = \"/home/baoanh/baoanh/DATN/ESFPNet/results/ESFP_B0_Endo_UTTQ_LA_1/UTTQ_Splited\"\n",
    "\n",
    "# List files in the folders\n",
    "mask_files = sorted(os.listdir(mask_folder))\n",
    "predicted_files = sorted(os.listdir(predicted_folder))\n",
    "\n",
    "# Initialize lists to store true and predicted values\n",
    "true_values = []\n",
    "predicted_values = []\n",
    "\n",
    "# Load and process masks and predicted images\n",
    "for mask_file, pred_file in zip(mask_files, predicted_files):\n",
    "    mask_path = os.path.join(mask_folder, mask_file)\n",
    "    pred_path = os.path.join(predicted_folder, pred_file)\n",
    "    \n",
    "    # Load images as arrays (grayscale images)\n",
    "    mask = np.array(Image.open(mask_path))\n",
    "    pred = np.array(Image.open(pred_path))\n",
    "    # print(pred.shape)\n",
    "    # print(mask.shape)\n",
    "    # Flatten images into 1D arrays\n",
    "    true_values.extend(mask.flatten())\n",
    "    predicted_values.extend(pred.flatten())\n",
    "    # break\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "true_values = np.array(true_values)/255 > 0.5\n",
    "predicted_values = np.array(predicted_values)/255 > 0.5\n",
    "print(np.unique(true_values))\n",
    "print(np.unique(predicted_values))\n",
    "# Calculate metrics\n",
    "precision = precision_score(true_values, predicted_values)\n",
    "recall = recall_score(true_values, predicted_values)\n",
    "f1 = f1_score(true_values, predicted_values)\n",
    "conf_matrix = confusion_matrix(true_values, predicted_values)\n",
    "\n",
    "# Calculate mIoU (Jaccard Index)\n",
    "jaccard_indices = jaccard_score(true_values, predicted_values, average=None)\n",
    "miou = np.mean(jaccard_indices)\n",
    "\n",
    "# Calculate mDice (Dice Coefficient)\n",
    "def dice_coef(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    total = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2.0 * intersection) / (total + 1e-6)\n",
    "\n",
    "mdice = dice_coef(true_values, predicted_values)\n",
    "\n",
    "# Calculate mSpecificity\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp + 1e-6)\n",
    "\n",
    "mspecificity = specificity_score(true_values, predicted_values)\n",
    "print(\"mDice:\", mdice)\n",
    "print(\"mIoU:\", miou)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"mSpecificity:\", mspecificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8099c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
