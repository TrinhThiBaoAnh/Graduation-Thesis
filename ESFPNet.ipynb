{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab7d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in and transforming data\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader,ConcatDataset\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import prune\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#from skimage import io, transform\n",
    "from PIL import Image\n",
    "\n",
    "# visualizing data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# load dataset information\n",
    "import yaml\n",
    "\n",
    "# image writing\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19c5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "WholeDatasetName = 'UTTQ'\n",
    "#WholeDatasetName = 'Kvasir'\n",
    "\n",
    "model_type = 'B0'\n",
    "_model_name = 'ESFP_{}_Endo_{}'.format(model_type,WholeDatasetName)\n",
    "config = open('Configure.yaml')\n",
    "config = yaml.safe_load(config)\n",
    "\n",
    "init_trainsize = 352\n",
    "batch_size = 8\n",
    "\n",
    "repeats = 1\n",
    "n_epochs = 200\n",
    "if_renew = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7b2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplittingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    dataloader for polyp segmentation tasks\n",
    "    \"\"\"\n",
    "    def __init__(self, image_root, gt_root):\n",
    "\n",
    "        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpeg') or f.endswith('.png')]\n",
    "        self.gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.png') or f.endswith('.jpeg')]\n",
    "        self.images = sorted(self.images)\n",
    "        self.gts = sorted(self.gts)\n",
    "        self.filter_files()\n",
    "        self.size = len(self.images)\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.rgb_loader(self.images[index])\n",
    "        gt = self.binary_loader(self.gts[index])\n",
    "        name = self.images[index].split('/')[-1]\n",
    "        return self.transform(image), self.transform(gt), name\n",
    "\n",
    "    def filter_files(self):\n",
    "        assert len(self.images) == len(self.gts)\n",
    "        images = []\n",
    "        gts = []\n",
    "        for img_path, gt_path in zip(self.images, self.gts):\n",
    "            img = Image.open(img_path)\n",
    "            gt = Image.open(gt_path)\n",
    "            if img.size == gt.size:\n",
    "                images.append(img_path)\n",
    "                gts.append(gt_path)\n",
    "        self.images = images\n",
    "        self.gts = gts\n",
    "\n",
    "    def rgb_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "\n",
    "    def binary_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('L')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3dcbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def devide_into_4_parts(image, gt):\n",
    "#     h, w = image.size()[2], image.size()[3]\n",
    "#     # print(h, w, type(image), type(gt), image.shape, gt.shape)\n",
    "#     sub_h = h // 2\n",
    "#     sub_w = w // 2\n",
    "\n",
    "#     # Create sub-images and sub-gts\n",
    "#     sub_images = [\n",
    "#         image[:, :, :sub_h, :sub_w],\n",
    "#         image[:, :, :sub_h, sub_w:],\n",
    "#         image[:, :, sub_h:, :sub_w],\n",
    "#         image[:, :, sub_h:, sub_w:]\n",
    "#     ]\n",
    "\n",
    "#     sub_gts = [\n",
    "#         gt[:,:, :sub_h, :sub_w],\n",
    "#         gt[:,:, :sub_h, sub_w:],\n",
    "#         gt[:,:, sub_h:, :sub_w],\n",
    "#         gt[:,:, sub_h:, sub_w:]\n",
    "#     ]\n",
    "#     return sub_images, sub_gts\n",
    "def splitDataset(renew, root_dir):\n",
    "    \n",
    "    split_train_images_save_path = './Endoscope-WL/{}_Splited/trainSplited/images'.format(WholeDatasetName)\n",
    "    os.makedirs(split_train_images_save_path, exist_ok=True)\n",
    "    split_train_masks_save_path = './Endoscope-WL/{}_Splited/trainSplited/masks'.format(WholeDatasetName)\n",
    "    os.makedirs(split_train_masks_save_path, exist_ok=True)\n",
    "    \n",
    "    split_validation_images_save_path = './Endoscope-WL/{}_Splited/validationSplited/images'.format(WholeDatasetName)\n",
    "    os.makedirs(split_validation_images_save_path, exist_ok=True)\n",
    "    split_validation_masks_save_path = './Endoscope-WL/{}_Splited/validationSplited/masks'.format(WholeDatasetName)\n",
    "    os.makedirs(split_validation_masks_save_path, exist_ok=True)\n",
    "    \n",
    "    split_test_images_save_path = './Endoscope-WL/{}_Splited/testSplited/images'.format(WholeDatasetName)\n",
    "    os.makedirs(split_test_images_save_path, exist_ok=True)\n",
    "    split_test_masks_save_path = './Endoscope-WL/{}_Splited/testSplited/masks'.format(WholeDatasetName)\n",
    "    os.makedirs(split_test_masks_save_path, exist_ok=True)\n",
    "    \n",
    "    if renew == True:\n",
    "    \n",
    "        DatasetList = []\n",
    "        \n",
    "        # images_train_path = config['dataset']['train_' + str(WholeDatasetName) + '_dataset'] + '/images/'\n",
    "        images_train_path = root_dir + \"/new_train/images/\"\n",
    "        # masks_train_path = config['dataset']['train_' + str(WholeDatasetName) + '_dataset'] + '/masks/'\n",
    "        masks_train_path = root_dir + \"/new_train/mask_images/\"\n",
    "        print(len(os.listdir(images_train_path)))\n",
    "        print(len(os.listdir(masks_train_path)))\n",
    "        Dataset_part_train = SplittingDataset(images_train_path, masks_train_path)\n",
    "        DatasetList.append(Dataset_part_train)\n",
    "         # images_test_path = config['dataset']['test_' + str(WholeDatasetName) + '_img']\n",
    "        images_val_path = root_dir + \"/new_valid/images/\"\n",
    "        # # masks_test_path = config['dataset']['test_' + str(WholeDatasetName) + '_label']\n",
    "        masks_val_path = root_dir + \"/new_valid/mask_images/\"\n",
    "        Dataset_part_val = SplittingDataset(images_val_path, masks_val_path)\n",
    "        DatasetList.append(Dataset_part_val)\n",
    "\n",
    "        # images_test_path = config['dataset']['test_' + str(WholeDatasetName) + '_img']\n",
    "        images_test_path = root_dir + \"/new_test/images/\"\n",
    "        # # masks_test_path = config['dataset']['test_' + str(WholeDatasetName) + '_label']\n",
    "        masks_test_path = root_dir + \"/new_test/mask_images/\"\n",
    "        Dataset_part_test = SplittingDataset(images_test_path, masks_test_path)\n",
    "        DatasetList.append(Dataset_part_test)\n",
    "                                    \n",
    "        wholeDataset = ConcatDataset([DatasetList[0], DatasetList[1], DatasetList[2]])\n",
    "        # val_num = int(0.1*len(wholeDataset))\n",
    "        # test_num = int(0.1*len(wholeDataset))\n",
    "                                    \n",
    "        # train_num = len(wholeDataset) - val_num - test_num\n",
    "        val_num = len(os.listdir(root_dir + \"/new_valid/images/\"))\n",
    "        # print(val_num)\n",
    "        # test_num = len(os.listdir(root_dir + \"/new_test/images/\"))\n",
    "        train_num = len(os.listdir(root_dir + \"/new_train/images/\"))\n",
    "        # print(train_num)\n",
    "        \n",
    "        # train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(wholeDataset, [train_num, val_num, test_num])\n",
    "        train_dataset = torch.utils.data.Subset(wholeDataset, range(train_num))\n",
    "        val_dataset = torch.utils.data.Subset(wholeDataset, range(train_num, train_num + val_num))\n",
    "        test_dataset = torch.utils.data.Subset(wholeDataset, range(train_num + val_num, len(wholeDataset)))\n",
    "                           \n",
    "        train_loader = DataLoader(dataset=train_dataset,batch_size=1,shuffle=False)\n",
    "        val_loader = DataLoader(dataset=val_dataset,batch_size=1,shuffle=False)\n",
    "        test_loader = DataLoader(dataset=test_dataset,batch_size=1,shuffle=False)\n",
    "    \n",
    "        iter_train = iter(train_loader)\n",
    "        for i in range(len(train_loader)):\n",
    "            image, gt, name = iter_train.next()\n",
    "            image_data = image.data.cpu().numpy().squeeze().transpose(1,2,0)\n",
    "            gt_data = gt.data.cpu().numpy().squeeze()\n",
    "            imageio.imwrite(split_train_images_save_path + '/' + name[0],img_as_ubyte(image_data))\n",
    "            imageio.imwrite(split_train_masks_save_path + '/' + name[0],img_as_ubyte(gt_data))\n",
    "            \n",
    "        iter_val = iter(val_loader)\n",
    "        for i in range(len(val_loader)):\n",
    "            image, gt, name = iter_val.next()\n",
    "            image_data = image.data.cpu().numpy().squeeze().transpose(1,2,0)\n",
    "            gt_data = gt.data.cpu().numpy().squeeze()\n",
    "            imageio.imwrite(split_validation_images_save_path + '/' + name[0],img_as_ubyte(image_data))\n",
    "            imageio.imwrite(split_validation_masks_save_path + '/' + name[0],img_as_ubyte(gt_data))\n",
    "            \n",
    "        \n",
    "        iter_test = iter(test_loader)\n",
    "        for i in range(len(test_loader)):\n",
    "            image, gt, name = iter_test.next()\n",
    "            image_data = image.data.cpu().numpy().squeeze().transpose(1,2,0)\n",
    "            gt_data = gt.data.cpu().numpy().squeeze()\n",
    "            imageio.imwrite(split_test_images_save_path + '/' + name[0],img_as_ubyte(image_data))\n",
    "            imageio.imwrite(split_test_masks_save_path + '/' + name[0],img_as_ubyte(gt_data))        \n",
    "    \n",
    "    return split_train_images_save_path, split_train_masks_save_path, split_validation_images_save_path, split_validation_masks_save_path, split_test_images_save_path, split_test_masks_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f6ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_renew = False\n",
    "root_dir = \"/home/baoanh/baoanh/DATN/dataset/Ungthuthucquan\"\n",
    "train_images_path, train_masks_path, val_images_path, val_masks_path, test_images_path, test_masks_path = splitDataset(if_renew, root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce8f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_path  = \"/home/baoanh/baoanh/DATN/ESFPNet/Endoscope-WL/UTTQ_Splited_original/\"\n",
    "# train_images_path = root_path + \"trainSplited/images\"\n",
    "# train_masks_path = root_path + \"trainSplited/masks\"\n",
    "# val_images_path = root_path + \"validationSplited/images\"\n",
    "# val_masks_path = root_path + \"validationSplited/masks\"\n",
    "# test_images_path = root_path + \"testSplited/images\"\n",
    "# test_masks_path = root_path + \"testSplited/masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e5a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolypDataset(Dataset):\n",
    "    \"\"\"\n",
    "    dataloader for polyp segmentation tasks\n",
    "    \"\"\"\n",
    "    def __init__(self, image_root, gt_root, trainsize, augmentations):\n",
    "        self.trainsize = trainsize\n",
    "        self.augmentations = augmentations\n",
    "        # print(self.augmentations)\n",
    "        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpeg') or f.endswith('.png')]\n",
    "        #print(image_root)\n",
    "        self.gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.png') or f.endswith('.jpeg')]\n",
    "        self.images = sorted(self.images)\n",
    "        self.gts = sorted(self.gts)\n",
    "        self.filter_files()\n",
    "        self.size = len(self.images)\n",
    "        if self.augmentations == True:\n",
    "            print('Using RandomRotation, RandomFlip')\n",
    "            self.img_transform = transforms.Compose([\n",
    "                transforms.RandomRotation(90, resample=False, expand=False, center=None),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.Resize((self.trainsize, self.trainsize)),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0, hue=0),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])])\n",
    "            self.gt_transform = transforms.Compose([\n",
    "                transforms.RandomRotation(90, resample=False, expand=False, center=None),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.Resize((self.trainsize, self.trainsize)),\n",
    "                transforms.ToTensor()])\n",
    "            \n",
    "        else:\n",
    "            print('no augmentation')\n",
    "            self.img_transform = transforms.Compose([\n",
    "                transforms.Resize((self.trainsize, self.trainsize)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])])\n",
    "            \n",
    "            self.gt_transform = transforms.Compose([\n",
    "                transforms.Resize((self.trainsize, self.trainsize)),\n",
    "                transforms.ToTensor()])\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = self.rgb_loader(self.images[index])\n",
    "        gt = self.binary_loader(self.gts[index])\n",
    "        \n",
    "        seed = np.random.randint(2147483647) # make a seed with numpy generator \n",
    "        np.random.seed(seed) # apply this seed to img tranfsorms\n",
    "        torch.manual_seed(seed) # needed for torchvision 0.7\n",
    "        if self.img_transform is not None:\n",
    "            image = self.img_transform(image)\n",
    "            \n",
    "        np.random.seed(seed) # apply this seed to img tranfsorms\n",
    "        torch.manual_seed(seed) # needed for torchvision 0.7\n",
    "        if self.gt_transform is not None:\n",
    "            gt = self.gt_transform(gt)\n",
    "        return image, gt\n",
    "\n",
    "    def filter_files(self):\n",
    "        assert len(self.images) == len(self.gts)\n",
    "        images = []\n",
    "        gts = []\n",
    "        for img_path, gt_path in zip(self.images, self.gts):\n",
    "            img = Image.open(img_path)\n",
    "            gt = Image.open(gt_path)\n",
    "            if img.size == gt.size:\n",
    "                images.append(img_path)\n",
    "                gts.append(gt_path)\n",
    "        self.images = images\n",
    "        self.gts = gts\n",
    "\n",
    "    def rgb_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "\n",
    "    def binary_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            # return img.convert('1')\n",
    "            return img.convert('L')\n",
    "\n",
    "    def resize(self, img, gt):\n",
    "        assert img.size == gt.size\n",
    "        w, h = img.size\n",
    "        if h < self.trainsize or w < self.trainsize:\n",
    "            h = max(h, self.trainsize)\n",
    "            w = max(w, self.trainsize)\n",
    "            return img.resize((w, h), Image.BILINEAR), gt.resize((w, h), Image.NEAREST)\n",
    "        else:\n",
    "            return img, gt\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "class test_dataset:\n",
    "    def __init__(self, image_root, gt_root, testsize):\n",
    "        self.testsize = testsize\n",
    "        self.images = [image_root + f for f in os.listdir(image_root) if f.endswith('.jpeg') or f.endswith('.png')]\n",
    "        self.gts = [gt_root + f for f in os.listdir(gt_root) if f.endswith('.tif') or f.endswith('.png') or f.endswith('.jpeg')]\n",
    "        self.images = sorted(self.images)\n",
    "        self.gts = sorted(self.gts)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((self.testsize, self.testsize)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                 [0.229, 0.224, 0.225])])\n",
    "        self.gt_transform = transforms.ToTensor()\n",
    "        self.size = len(self.images)\n",
    "        self.index = 0\n",
    "\n",
    "    def load_data(self):\n",
    "        image = self.rgb_loader(self.images[self.index])\n",
    "        image = self.transform(image).unsqueeze(0)\n",
    "        gt = self.binary_loader(self.gts[self.index])\n",
    "        name = self.images[self.index].split('/')[-1]\n",
    "        if name.endswith('.jpg'):\n",
    "            name = name.split('.jpg')[0] + '.png'\n",
    "        self.index += 1\n",
    "        return image, gt, name\n",
    "\n",
    "    def rgb_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "\n",
    "    def binary_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc993709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoder import mit\n",
    "from Decoder import mlp\n",
    "from mmcv.cnn import ConvModule\n",
    "\n",
    "class ESFPNetStructure(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim = 160):\n",
    "        super(ESFPNetStructure, self).__init__()\n",
    "        \n",
    "        # Backbone\n",
    "        if model_type == 'B0':\n",
    "            self.backbone = mit.mit_b0()\n",
    "        if model_type == 'B1':\n",
    "            self.backbone = mit.mit_b1()\n",
    "        if model_type == 'B2':\n",
    "            self.backbone = mit.mit_b2()\n",
    "        if model_type == 'B3':\n",
    "            self.backbone = mit.mit_b3()\n",
    "        if model_type == 'B4':\n",
    "            self.backbone = mit.mit_b4()\n",
    "        if model_type == 'B5':\n",
    "            self.backbone = mit.mit_b5()\n",
    "        self.l1_lambda = 1e-6\n",
    "        self.l2_lambda = 1e-3\n",
    "        self._init_weights()  # load pretrain\n",
    "        \n",
    "        # LP Header\n",
    "        self.LP_1 = mlp.LP(input_dim = self.backbone.embed_dims[0], embed_dim = self.backbone.embed_dims[0])\n",
    "        self.LP_2 = mlp.LP(input_dim = self.backbone.embed_dims[1], embed_dim = self.backbone.embed_dims[1])\n",
    "        self.LP_3 = mlp.LP(input_dim = self.backbone.embed_dims[2], embed_dim = self.backbone.embed_dims[2])\n",
    "        self.LP_4 = mlp.LP(input_dim = self.backbone.embed_dims[3], embed_dim = self.backbone.embed_dims[3])\n",
    "        \n",
    "        # Linear Fuse\n",
    "        self.linear_fuse34 = ConvModule(in_channels=(self.backbone.embed_dims[2] + self.backbone.embed_dims[3]), out_channels=self.backbone.embed_dims[2], kernel_size=1,norm_cfg=dict(type='BN', requires_grad=True))\n",
    "        self.linear_fuse23 = ConvModule(in_channels=(self.backbone.embed_dims[1] + self.backbone.embed_dims[2]), out_channels=self.backbone.embed_dims[1], kernel_size=1,norm_cfg=dict(type='BN', requires_grad=True))\n",
    "        self.linear_fuse12 = ConvModule(in_channels=(self.backbone.embed_dims[0] + self.backbone.embed_dims[1]), out_channels=self.backbone.embed_dims[0], kernel_size=1,norm_cfg=dict(type='BN', requires_grad=True))\n",
    "        \n",
    "        # Fused LP Header\n",
    "        self.LP_12 = mlp.LP(input_dim = self.backbone.embed_dims[0], embed_dim = self.backbone.embed_dims[0])\n",
    "        self.LP_23 = mlp.LP(input_dim = self.backbone.embed_dims[1], embed_dim = self.backbone.embed_dims[1])\n",
    "        self.LP_34 = mlp.LP(input_dim = self.backbone.embed_dims[2], embed_dim = self.backbone.embed_dims[2])\n",
    "        \n",
    "        # Final Linear Prediction\n",
    "        self.linear_pred = nn.Conv2d((self.backbone.embed_dims[0] + self.backbone.embed_dims[1] + self.backbone.embed_dims[2] + self.backbone.embed_dims[3]), 1, kernel_size=1)\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \n",
    "        if model_type == 'B0':\n",
    "            pretrained_dict = torch.load('./Pretrained/mit_b0.pth')\n",
    "        if model_type == 'B1':\n",
    "            pretrained_dict = torch.load('./Pretrained/mit_b1.pth')\n",
    "        if model_type == 'B2':\n",
    "            pretrained_dict = torch.load('./Pretrained/mit_b2.pth')\n",
    "        if model_type == 'B3':\n",
    "            pretrained_dict = torch.load('./Pretrained/mit_b3.pth')\n",
    "        if model_type == 'B4':\n",
    "            pretrained_dict = torch.load('./Pretrained/mit_b4.pth')\n",
    "        if model_type == 'B5':\n",
    "            pretrained_dict = torch.load('./Pretrained/mit_b5.pth')\n",
    "            \n",
    "            \n",
    "        model_dict = self.backbone.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.backbone.load_state_dict(model_dict)\n",
    "        print(\"successfully loaded!!!!\")\n",
    "        \n",
    "    # def l1_regularization_loss(self):\n",
    "    #     l1_loss = 0.0\n",
    "    #     # print(self.parameters())\n",
    "    #     for param in self.parameters():\n",
    "    #         # print(param)\n",
    "    #         l1_loss += torch.sum(torch.abs(param))\n",
    "    #     return self.l1_lambda * l1_loss  \n",
    "    \n",
    "    # def l2_regularization_loss(self):\n",
    "    #     l2_loss = 0.0\n",
    "    #     for param in self.parameters():\n",
    "    #         l2_loss += torch.sum(param ** 2)\n",
    "    #     return 0.5 * self.l2_lambda * l2_loss\n",
    "    # def smooth_lasso_regularization_loss(self):\n",
    "    #     smooth_lasso_loss = 0.0\n",
    "    #     # print(self.parameters())\n",
    "    #     for param in self.parameters():\n",
    "    #         # print(param)\n",
    "    #         if torch.sum(torch.abs(param)) < 1:\n",
    "    #             smooth_lasso_loss += 0.5*torch.sum(param ** 2)\n",
    "    #         else:\n",
    "    #             smooth_lasso_loss += torch.sum(torch.abs(param)) - 0.5\n",
    "    #     return self.l1_lambda * smooth_lasso_loss\n",
    "    def forward(self, x):\n",
    "        \n",
    "        ##################  Go through backbone ###################\n",
    "        \n",
    "        B = x.shape[0]\n",
    "        \n",
    "        #stage 1\n",
    "        out_1, H, W = self.backbone.patch_embed1(x)\n",
    "        for i, blk in enumerate(self.backbone.block1):\n",
    "            out_1 = blk(out_1, H, W)\n",
    "        out_1 = self.backbone.norm1(out_1)\n",
    "        out_1 = out_1.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()  #(Batch_Size, self.backbone.embed_dims[0], 88, 88)\n",
    "        \n",
    "        # stage 2\n",
    "        out_2, H, W = self.backbone.patch_embed2(out_1)\n",
    "        for i, blk in enumerate(self.backbone.block2):\n",
    "            out_2 = blk(out_2, H, W)\n",
    "        out_2 = self.backbone.norm2(out_2)\n",
    "        out_2 = out_2.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()  #(Batch_Size, self.backbone.embed_dims[1], 44, 44)\n",
    "        \n",
    "        # stage 3\n",
    "        out_3, H, W = self.backbone.patch_embed3(out_2)\n",
    "        for i, blk in enumerate(self.backbone.block3):\n",
    "            out_3 = blk(out_3, H, W)\n",
    "        out_3 = self.backbone.norm3(out_3)\n",
    "        out_3 = out_3.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()  #(Batch_Size, self.backbone.embed_dims[2], 22, 22)\n",
    "        \n",
    "        # stage 4\n",
    "        out_4, H, W = self.backbone.patch_embed4(out_3)\n",
    "        for i, blk in enumerate(self.backbone.block4):\n",
    "            out_4 = blk(out_4, H, W)\n",
    "        out_4 = self.backbone.norm4(out_4)\n",
    "        out_4 = out_4.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()  #(Batch_Size, self.backbone.embed_dims[3], 11, 11)\n",
    "        \n",
    "        # go through LP Header\n",
    "        lp_1 = self.LP_1(out_1)\n",
    "        lp_2 = self.LP_2(out_2)  \n",
    "        lp_3 = self.LP_3(out_3)  \n",
    "        lp_4 = self.LP_4(out_4)\n",
    "        \n",
    "        # linear fuse and go pass LP Header\n",
    "        lp_34 = self.LP_34(self.linear_fuse34(torch.cat([lp_3, F.interpolate(lp_4,scale_factor=2,mode='bilinear', align_corners=False)], dim=1)))\n",
    "        lp_23 = self.LP_23(self.linear_fuse23(torch.cat([lp_2, F.interpolate(lp_34,scale_factor=2,mode='bilinear', align_corners=False)], dim=1)))\n",
    "        lp_12 = self.LP_12(self.linear_fuse12(torch.cat([lp_1, F.interpolate(lp_23,scale_factor=2,mode='bilinear', align_corners=False)], dim=1)))\n",
    "        \n",
    "        # get the final output\n",
    "        lp4_resized = F.interpolate(lp_4,scale_factor=8,mode='bilinear', align_corners=False)\n",
    "        lp3_resized = F.interpolate(lp_34,scale_factor=4,mode='bilinear', align_corners=False)\n",
    "        lp2_resized = F.interpolate(lp_23,scale_factor=2,mode='bilinear', align_corners=False)\n",
    "        lp1_resized = lp_12\n",
    "        \n",
    "        out = self.linear_pred(torch.cat([lp1_resized, lp2_resized, lp3_resized, lp4_resized], dim=1))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e858b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# def focal_loss(pred, mask, gamma=2, alpha=0.25):\n",
    "#     # Calculate BCE Loss\n",
    "#     # bce = F.binary_cross_entropy_with_logits(pred, mask, reduction='none')\n",
    "\n",
    "#     # # Calculate the modulating factor\n",
    "#     # p_t = torch.exp(-bce)\n",
    "#     # modulating_factor = (1 - p_t) ** gamma\n",
    "\n",
    "#     # # Calculate the Focal Loss\n",
    "    \n",
    "#     # focal_loss = modulating_factor * bce\n",
    "\n",
    "#     # # Calculate the weighted focal loss\n",
    "#     # weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=15, stride=1, padding=7) - mask)\n",
    "#     # weighted_focal_loss = (weit * focal_loss).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "#     # # print(\"weighted_focal_loss: \", weighted_focal_loss)\n",
    "#     # return weighted_focal_loss.mean()\n",
    "#     mask = mask.float()\n",
    "#     pt = torch.sigmoid(pred) * mask + (1 - torch.sigmoid(pred)) * (1 - mask)\n",
    "#     loss = -alpha * (1 - pt).pow(gamma) * torch.log(pt + 1e-12) - (1 - alpha) * pt.pow(gamma) * torch.log(1 - pt + 1e-12)\n",
    "\n",
    "#     return loss.sum()\n",
    "\n",
    "def focal_loss(pred, target, gamma=2, alpha=0.25, smooth=0.0001):\n",
    "    num = target.size(0)\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    dice_coefficient = (2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)\n",
    "\n",
    "    # Calculate the focal weights for each pixel\n",
    "    focal_weights = torch.pow(1 - dice_coefficient, gamma)\n",
    "\n",
    "    # Calculate the focal loss (element-wise multiplication with alpha)\n",
    "    focal_loss = -alpha * torch.log(dice_coefficient) * focal_weights\n",
    "\n",
    "    return focal_loss.sum() / num\n",
    "def ange_structure_loss(pred, mask, smooth=1):\n",
    "    \n",
    "    weit = 1 + 5*torch.abs(F.avg_pool2d(mask, kernel_size=15, stride=1, padding=7) - mask)\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduction='mean')\n",
    "    wbce = (weit*wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask)*weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask)*weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + smooth)/(union - inter + smooth)\n",
    "\n",
    "    wfocal = focal_loss(pred, mask) \n",
    "\n",
    "    # return (wbce+wiou).mean()\n",
    "    return (wiou + wfocal + wbce).mean()\n",
    "\n",
    "def dice_loss_coff(pred, target, smooth = 0.0001):\n",
    "    \n",
    "    num = target.size(0)\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)\n",
    "    \n",
    "    return loss.sum()/num\n",
    "def tversky_loss(pred, target, alpha=0.7, beta=0.3, smooth=0.0001):\n",
    "    num = target.size(0)\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    false_positive = (pred * (1 - target)).sum(dim=2).sum(dim=2)\n",
    "    false_negative = ((1 - pred) * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    tversky = (intersection + smooth) / (intersection + alpha * false_positive + beta * false_negative + smooth)\n",
    "    loss = 1 - tversky\n",
    "    \n",
    "    return loss.sum() / num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c112d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def evaluate():  \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ESFPNet.eval()\n",
    "    \n",
    "    val = 0\n",
    "    count = 0\n",
    "\n",
    "    smooth = 1e-4\n",
    "    \n",
    "    val_loader = test_dataset(val_images_path + '/',val_masks_path + '/', init_trainsize)\n",
    "    for i in range(val_loader.size):\n",
    "        image, gt, name = val_loader.load_data()\n",
    "        gt = np.asarray(gt, np.float32)\n",
    "        gt /= (gt.max() + 1e-8)\n",
    "\n",
    "        image = image.cuda()\n",
    "        \n",
    "        pred= ESFPNet(image)\n",
    "        pred = F.upsample(pred, size=gt.shape, mode='bilinear', align_corners=False)\n",
    "        pred = pred.sigmoid()\n",
    "        threshold = torch.tensor([0.5]).to(device)\n",
    "        pred = (pred > threshold).float() * 1\n",
    "\n",
    "        pred = pred.data.cpu().numpy().squeeze()\n",
    "        pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "        \n",
    "        target = np.array(gt)\n",
    "        \n",
    "        input_flat = np.reshape(pred,(-1))\n",
    "        target_flat = np.reshape(target,(-1))\n",
    " \n",
    "        intersection = (input_flat*target_flat)\n",
    "        \n",
    "        loss =  (2 * intersection.sum() + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "\n",
    "        a =  '{:.4f}'.format(loss)\n",
    "        a = float(a)\n",
    "        \n",
    "        val = val + a\n",
    "        count = count + 1\n",
    "        \n",
    "    ESFPNet.train()\n",
    "    \n",
    "    return val/count, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8eb84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_non_zero_parameters(model):\n",
    "#     total_non_zero_params = 0\n",
    "#     for param in model.parameters():\n",
    "#         total_non_zero_params += torch.sum(param != 0).item()\n",
    "#     return total_non_zero_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ce57ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "def training_loop(n_epochs,model, ESFPNet_optimizer, numIters):\n",
    "    \n",
    "    # keep track of losses over time\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    coeff_max = 0\n",
    "    \n",
    "    # set up data and then train\n",
    "    trainDataset = PolypDataset(train_images_path + '/', train_masks_path + '/', trainsize=init_trainsize, augmentations = True)\n",
    "    train_loader = DataLoader(dataset=trainDataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    iter_X = iter(train_loader)\n",
    "    steps_per_epoch = len(iter_X)\n",
    "    num_epoch = 0\n",
    "    total_steps = (n_epochs+1)*steps_per_epoch\n",
    "    num_to_stop = 0\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for step in range(1, total_steps):\n",
    "\n",
    "        # Reset iterators for each epoch\n",
    "        if step % steps_per_epoch == 0:\n",
    "            iter_X = iter(train_loader)\n",
    "            num_epoch = num_epoch + 1\n",
    "        \n",
    "        # make sure to scale to a range -1 to 1\n",
    "        images, masks = iter_X.next()\n",
    "        \n",
    "        # move images to GPU if available (otherwise stay on CPU)\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "\n",
    "        # ============================================\n",
    "        #            TRAIN THE NETWORKS\n",
    "        # ============================================\n",
    "       \n",
    "        ESFPNet_optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Compute the losses from the network\n",
    "        \n",
    "        out = ESFPNet(images)\n",
    "        out = F.interpolate(out, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        l1_regularization_strength = 1e-3\n",
    "        loss = ange_structure_loss(out, masks)\n",
    "        # print(\"Loss: \", loss.item())\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        # l1_reg = torch.tensor(0.0).to(device)\n",
    "        # for module in model.modules():\n",
    "        #     # mask = None\n",
    "        #     # weight = None\n",
    "        #     # for name, buffer in module.named_buffers():\n",
    "        #     #     if name == \"weight_mask\":\n",
    "        #     #         mask = buffer\n",
    "        #     for name, param in module.named_parameters():\n",
    "        #         if name == \"weight_orig\":\n",
    "        #             # weight = param\n",
    "        #             l1_reg += torch.norm(param, 1)\n",
    "        #     # We usually only want to introduce sparsity to weights and prune weights.\n",
    "        #     # Do the same for bias if necessary.\n",
    "        #     # if mask is not None and weight is not None:\n",
    "        #     #     l1_reg += torch.norm(mask * weight, 1)\n",
    "        \n",
    "        # loss += l1_regularization_strength * l1_reg \n",
    "        # loss += model.l1_regularization()  \n",
    "    \n",
    "        loss.backward()\n",
    "        ESFPNet_optimizer.step() \n",
    "        \n",
    "        # ============================================\n",
    "        #            TRAIN THE NETWORKS\n",
    "        # ============================================\n",
    "        # Print the log info\n",
    "        if step % steps_per_epoch == 0:\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            print('Epoch [{:5d}/{:5d}] | preliminary loss: {:6.6f} '.format(num_epoch, n_epochs, loss.item()))\n",
    "            \n",
    "        if step % steps_per_epoch == 0:\n",
    "            \n",
    "            validation_coeff, val_loss = evaluate()\n",
    "            val_losses.append(val_loss.item())\n",
    "            print('Epoch [{:5d}/{:5d}] | validation_coeffient: {:6.6f} '.format(\n",
    "                    num_epoch, n_epochs, validation_coeff))\n",
    "            \n",
    "            if coeff_max < validation_coeff:\n",
    "                coeff_max = validation_coeff\n",
    "                save_model_path = './SaveModel/{}_LA_{:1d}'.format(_model_name,numIters)\n",
    "                os.makedirs(save_model_path, exist_ok=True)\n",
    "                # print(save_model_path)\n",
    "                torch.save(ESFPNet, save_model_path + '/ESFPNet.pt')\n",
    "                print('Save Learning Ability Optimized Model at Epoch [{:5d}/{:5d}]'.format(num_epoch, n_epochs))\n",
    "                num_to_stop = 0\n",
    "            else:\n",
    "                num_to_stop = num_to_stop + 1\n",
    "    \n",
    "        if num_to_stop >=10:\n",
    "            break\n",
    "    return losses, val_losses, coeff_max, num_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d06570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResult(numIters):\n",
    "    \n",
    "    save_path = './results/{}_LA_{:1d}/{}_Splited/'.format(_model_name,numIters,str(WholeDatasetName))\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    print(save_path)\n",
    "    device = torch.device(\"cuda:0\")    \n",
    "    model_path =  './SaveModel/{}_LA_{:1d}'.format(_model_name,numIters)\n",
    "    ESFPNetBest = torch.load(model_path + '/ESFPNet.pt')\n",
    "    ESFPNetBest.eval()\n",
    "    \n",
    "    test_loader = test_dataset(test_images_path + '/', test_masks_path + '/', init_trainsize)\n",
    "    for i in range(test_loader.size):\n",
    "        image, gt, name = test_loader.load_data()\n",
    "        gt = np.asarray(gt, np.float32)\n",
    "        gt /= (gt.max() + 1e-8)\n",
    "        image = image.cuda()\n",
    "\n",
    "        pred = ESFPNetBest(image)\n",
    "        pred = F.interpolate(pred, size=gt.shape, mode='bilinear', align_corners=False)\n",
    "        pred = pred.sigmoid()\n",
    "        threshold = torch.tensor([0.5]).to(device)\n",
    "        pred = (pred > threshold).float() * 1\n",
    "        pred = pred.data.cpu().numpy().squeeze()\n",
    "        pred = (pred - pred.min()) / (pred.max() - pred.min() + 1e-8)\n",
    "        \n",
    "        imageio.imwrite(save_path+name,img_as_ubyte(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "022af04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESFPNet = ESFPNetStructure()\n",
    "# print(ESFPNet)\n",
    "# # def count_parameters(model):\n",
    "# #     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # # Example usage:\n",
    "# # model = ESFPNetStructure()  # Initialize your model\n",
    "# # total_params = count_parameters(model)\n",
    "# # print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "114565ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded!!!!\n",
      "Models moved to GPU.\n",
      "#####################################################################################\n",
      "Using RandomRotation, RandomFlip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baoanh/miniconda3/envs/esfp/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1292: UserWarning: The parameter 'resample' is deprecated since 0.12 and will be removed 0.14. Please use 'interpolation' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [    1/  200] | preliminary loss: 0.856788 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baoanh/miniconda3/envs/esfp/lib/python3.10/site-packages/torch/nn/functional.py:3704: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [    1/  200] | validation_coeffient: 0.720655 \n",
      "Save Learning Ability Optimized Model at Epoch [    1/  200]\n",
      "Epoch [    2/  200] | preliminary loss: 0.835259 \n",
      "Epoch [    2/  200] | validation_coeffient: 0.762238 \n",
      "Save Learning Ability Optimized Model at Epoch [    2/  200]\n",
      "Epoch [    3/  200] | preliminary loss: 0.591232 \n",
      "Epoch [    3/  200] | validation_coeffient: 0.763592 \n",
      "Save Learning Ability Optimized Model at Epoch [    3/  200]\n",
      "Epoch [    4/  200] | preliminary loss: 0.668249 \n",
      "Epoch [    4/  200] | validation_coeffient: 0.773958 \n",
      "Save Learning Ability Optimized Model at Epoch [    4/  200]\n",
      "Epoch [    5/  200] | preliminary loss: 0.421251 \n",
      "Epoch [    5/  200] | validation_coeffient: 0.794489 \n",
      "Save Learning Ability Optimized Model at Epoch [    5/  200]\n",
      "Epoch [    6/  200] | preliminary loss: 0.643413 \n",
      "Epoch [    6/  200] | validation_coeffient: 0.796374 \n",
      "Save Learning Ability Optimized Model at Epoch [    6/  200]\n",
      "Epoch [    7/  200] | preliminary loss: 0.967525 \n",
      "Epoch [    7/  200] | validation_coeffient: 0.777987 \n",
      "Epoch [    8/  200] | preliminary loss: 0.673323 \n",
      "Epoch [    8/  200] | validation_coeffient: 0.782400 \n",
      "Epoch [    9/  200] | preliminary loss: 0.610027 \n",
      "Epoch [    9/  200] | validation_coeffient: 0.790817 \n",
      "Epoch [   10/  200] | preliminary loss: 0.574807 \n",
      "Epoch [   10/  200] | validation_coeffient: 0.800375 \n",
      "Save Learning Ability Optimized Model at Epoch [   10/  200]\n",
      "Epoch [   11/  200] | preliminary loss: 0.616344 \n",
      "Epoch [   11/  200] | validation_coeffient: 0.810906 \n",
      "Save Learning Ability Optimized Model at Epoch [   11/  200]\n",
      "Epoch [   12/  200] | preliminary loss: 0.560202 \n",
      "Epoch [   12/  200] | validation_coeffient: 0.814196 \n",
      "Save Learning Ability Optimized Model at Epoch [   12/  200]\n",
      "Epoch [   13/  200] | preliminary loss: 0.725039 \n",
      "Epoch [   13/  200] | validation_coeffient: 0.792309 \n",
      "Epoch [   14/  200] | preliminary loss: 0.600390 \n",
      "Epoch [   14/  200] | validation_coeffient: 0.814591 \n",
      "Save Learning Ability Optimized Model at Epoch [   14/  200]\n",
      "Epoch [   15/  200] | preliminary loss: 0.482157 \n",
      "Epoch [   15/  200] | validation_coeffient: 0.815694 \n",
      "Save Learning Ability Optimized Model at Epoch [   15/  200]\n",
      "Epoch [   16/  200] | preliminary loss: 0.628019 \n",
      "Epoch [   16/  200] | validation_coeffient: 0.817125 \n",
      "Save Learning Ability Optimized Model at Epoch [   16/  200]\n",
      "Epoch [   17/  200] | preliminary loss: 0.582598 \n",
      "Epoch [   17/  200] | validation_coeffient: 0.821889 \n",
      "Save Learning Ability Optimized Model at Epoch [   17/  200]\n",
      "Epoch [   18/  200] | preliminary loss: 0.427641 \n",
      "Epoch [   18/  200] | validation_coeffient: 0.806977 \n",
      "Epoch [   19/  200] | preliminary loss: 0.504237 \n",
      "Epoch [   19/  200] | validation_coeffient: 0.819379 \n",
      "Epoch [   20/  200] | preliminary loss: 0.397227 \n",
      "Epoch [   20/  200] | validation_coeffient: 0.817166 \n",
      "Epoch [   21/  200] | preliminary loss: 0.434716 \n",
      "Epoch [   21/  200] | validation_coeffient: 0.816642 \n",
      "Epoch [   22/  200] | preliminary loss: 0.446153 \n",
      "Epoch [   22/  200] | validation_coeffient: 0.821179 \n",
      "Epoch [   23/  200] | preliminary loss: 0.457965 \n",
      "Epoch [   23/  200] | validation_coeffient: 0.809234 \n",
      "Epoch [   24/  200] | preliminary loss: 0.499350 \n",
      "Epoch [   24/  200] | validation_coeffient: 0.827753 \n",
      "Save Learning Ability Optimized Model at Epoch [   24/  200]\n",
      "Epoch [   25/  200] | preliminary loss: 0.391962 \n",
      "Epoch [   25/  200] | validation_coeffient: 0.837547 \n",
      "Save Learning Ability Optimized Model at Epoch [   25/  200]\n",
      "Epoch [   26/  200] | preliminary loss: 0.405982 \n",
      "Epoch [   26/  200] | validation_coeffient: 0.823464 \n",
      "Epoch [   27/  200] | preliminary loss: 0.353152 \n",
      "Epoch [   27/  200] | validation_coeffient: 0.828332 \n",
      "Epoch [   28/  200] | preliminary loss: 0.448931 \n",
      "Epoch [   28/  200] | validation_coeffient: 0.832826 \n",
      "Epoch [   29/  200] | preliminary loss: 0.403193 \n",
      "Epoch [   29/  200] | validation_coeffient: 0.815553 \n",
      "Epoch [   30/  200] | preliminary loss: 0.430882 \n",
      "Epoch [   30/  200] | validation_coeffient: 0.826275 \n",
      "Epoch [   31/  200] | preliminary loss: 0.418717 \n",
      "Epoch [   31/  200] | validation_coeffient: 0.830423 \n",
      "Epoch [   32/  200] | preliminary loss: 0.316767 \n",
      "Epoch [   32/  200] | validation_coeffient: 0.821374 \n",
      "Epoch [   33/  200] | preliminary loss: 0.319121 \n",
      "Epoch [   33/  200] | validation_coeffient: 0.843677 \n",
      "Save Learning Ability Optimized Model at Epoch [   33/  200]\n",
      "Epoch [   34/  200] | preliminary loss: 0.373778 \n",
      "Epoch [   34/  200] | validation_coeffient: 0.821872 \n",
      "Epoch [   35/  200] | preliminary loss: 0.321913 \n",
      "Epoch [   35/  200] | validation_coeffient: 0.827742 \n",
      "Epoch [   36/  200] | preliminary loss: 0.460139 \n",
      "Epoch [   36/  200] | validation_coeffient: 0.839242 \n",
      "Epoch [   37/  200] | preliminary loss: 0.317309 \n",
      "Epoch [   37/  200] | validation_coeffient: 0.830774 \n",
      "Epoch [   38/  200] | preliminary loss: 0.325980 \n",
      "Epoch [   38/  200] | validation_coeffient: 0.831300 \n",
      "Epoch [   39/  200] | preliminary loss: 0.515057 \n",
      "Epoch [   39/  200] | validation_coeffient: 0.836196 \n",
      "Epoch [   40/  200] | preliminary loss: 0.383896 \n",
      "Epoch [   40/  200] | validation_coeffient: 0.815240 \n",
      "Epoch [   41/  200] | preliminary loss: 0.450530 \n",
      "Epoch [   41/  200] | validation_coeffient: 0.830628 \n",
      "Epoch [   42/  200] | preliminary loss: 0.244888 \n",
      "Epoch [   42/  200] | validation_coeffient: 0.838153 \n",
      "Epoch [   43/  200] | preliminary loss: 0.392596 \n",
      "Epoch [   43/  200] | validation_coeffient: 0.833147 \n",
      "#####################################################################################\n",
      "optimize_m_dice: 0.843677\n",
      "./results/ESFP_B0_Endo_UTTQ_LA_1/UTTQ_Splited/\n",
      "#####################################################################################\n",
      "saved the results\n",
      "#####################################################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "def measure_global_sparsity(model,\n",
    "                            weight=True,\n",
    "                            bias=False,\n",
    "                            conv2d_use_mask=False,\n",
    "                            linear_use_mask=False):\n",
    "\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "\n",
    "    for module_name, module in model.named_modules():\n",
    "\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
    "                module, weight=weight, bias=bias, use_mask=conv2d_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "        elif isinstance(module, torch.nn.Linear):\n",
    "\n",
    "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
    "                module, weight=weight, bias=bias, use_mask=linear_use_mask)\n",
    "            num_zeros += module_num_zeros\n",
    "            num_elements += module_num_elements\n",
    "\n",
    "    sparsity = num_zeros / num_elements\n",
    "\n",
    "    return num_zeros, num_elements, sparsity\n",
    "def measure_module_sparsity(module, weight=True, bias=False, use_mask=False):\n",
    "\n",
    "    num_zeros = 0\n",
    "    num_elements = 0\n",
    "\n",
    "    if use_mask == True:\n",
    "        for buffer_name, buffer in module.named_buffers():\n",
    "            if \"weight_mask\" in buffer_name and weight == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "            if \"bias_mask\" in buffer_name and bias == True:\n",
    "                num_zeros += torch.sum(buffer == 0).item()\n",
    "                num_elements += buffer.nelement()\n",
    "    else:\n",
    "        for param_name, param in module.named_parameters():\n",
    "            if \"weight\" in param_name and weight == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "            if \"bias\" in param_name and bias == True:\n",
    "                num_zeros += torch.sum(param == 0).item()\n",
    "                num_elements += param.nelement()\n",
    "\n",
    "    sparsity = num_zeros / num_elements\n",
    "\n",
    "    return num_zeros, num_elements, sparsity\n",
    "for i in range(repeats):\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ESFPNet = ESFPNetStructure()\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        ESFPNet.to(device)\n",
    "        print('Models moved to GPU.')\n",
    "    else:\n",
    "        print('Only CPU available.')\n",
    "    print('#####################################################################################')  \n",
    "    # for module_name, module in ESFPNet.named_modules():\n",
    "    #     # print(module_name)\n",
    "    #     if isinstance(module, torch.nn.Conv2d):\n",
    "    #         #--conv2d_prune_amount\n",
    "    #         # prune.l1_unstructured(\n",
    "    #         #             module, name=\"weight\", amount=0.01\n",
    "    #         # )\n",
    "    #         pass\n",
    "    #     elif \"LP\" in module_name and isinstance(module, torch.nn.Linear):\n",
    "    #         #--linear_prune_amount\n",
    "    #         prune.l1_unstructured(\n",
    "    #                     module, name=\"weight\", amount=0.2\n",
    "    #         )\n",
    "    # num_zeros, num_elements, sparsity = measure_global_sparsity(\n",
    "    #         ESFPNet, weight=True, bias=False, linear_use_mask=False\n",
    "    #     )\n",
    "    # print(f\"Model sparsity at first: {sparsity}\")\n",
    "    # hyperparams for Adam optimizer\n",
    "    lr=0.0001 #0.0001\n",
    "\n",
    "    ESFPNet_optimizer = optim.AdamW(ESFPNet.parameters(), lr=lr)\n",
    "\n",
    "    \n",
    "    losses, val_losses, coeff_max, num_epoch = training_loop(n_epochs,ESFPNet, ESFPNet_optimizer, i+1)\n",
    "    # num_zeros, num_elements, sparsity = measure_global_sparsity(\n",
    "    #     ESFPNet, weight=True, bias=False, conv2d_use_mask=True, linear_use_mask=False\n",
    "    # )\n",
    "    # print(f\"Model sparsity at last: {sparsity}\")\n",
    "    plt.plot(range(1, num_epoch+1), losses, color='b', label='Training Loss')\n",
    "    plt.plot(range(1, num_epoch+1), val_losses, color='r', label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    print('#####################################################################################')  \n",
    "    print('optimize_m_dice: {:6.6f}'.format(coeff_max))\n",
    "    plt.savefig(f\"visualize_{str(WholeDatasetName)}.png\")\n",
    "    saveResult(i+1)\n",
    "    print('#####################################################################################')  \n",
    "    print('saved the results')\n",
    "    print('#####################################################################################')  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "895f0550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 55\n",
      "mDice: 0.8486395217793089\n",
      "mIoU: 0.8137707401085194\n",
      "Recall: 0.8210906904208645\n",
      "Precision: 0.8781011397174372\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from PIL import Image\n",
    "from sklearn.metrics import jaccard_score, f1_score, confusion_matrix\n",
    "from skimage import img_as_bool\n",
    "from PIL import Image\n",
    "# WholeDatasetName =\"UTTQ\"\n",
    "# Paths to folders containing masks and predicted images\n",
    "mask_folder = f\"/home/baoanh/baoanh/DATN/ESFPNet/Endoscope-WL/{WholeDatasetName}_Splited/testSplited/masks\"\n",
    "# mask_folder = \"/home/baoanh/baoanh/DATN/dataset/7 Loét HTT/new_test/mask_images\"\n",
    "predicted_folder = f\"/home/baoanh/baoanh/DATN/ESFPNet/results/ESFP_B0_Endo_{WholeDatasetName}_LA_1/{WholeDatasetName}_Splited\"\n",
    "# predicted_folder = \"/home/baoanh/baoanh/DATN/ESFPNet/results/ESFP_B0_Endo_UTTQ_LA_1/UTTQ_Splited\"\n",
    "# mask_folder = \"/home/baoanh/baoanh/DATN/dataset/ViemdadayHpDuong/test/mask_images\"\n",
    "# List files in the folders\n",
    "mask_files = os.listdir(mask_folder)\n",
    "predicted_files = os.listdir(predicted_folder)\n",
    "print(len(mask_files), len(predicted_files))\n",
    "# Initialize lists to store true and predicted values\n",
    "true_values = []\n",
    "predicted_values = []\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp + 1e-6)\n",
    "precision = 0\n",
    "recall = 0\n",
    "f1 = 0\n",
    "iou = 0\n",
    "specificity = 0\n",
    "# Load and process masks and predicted images\n",
    "for mask_file, pred_file in zip(mask_files, predicted_files):\n",
    "    mask_path = os.path.join(mask_folder, mask_file)\n",
    "    pred_path = os.path.join(predicted_folder, pred_file)\n",
    "    \n",
    "    # Load images as arrays (grayscale images)\n",
    "    mask = np.array(Image.open(mask_path))\n",
    "    pred = np.array(Image.open(pred_path))\n",
    "    # print(mask.shape, pred.shape, np.unique(mask), np.unique(pred))\n",
    "    # mask = np.array(mask)/255 > 0.5\n",
    "    # pred = np.array(pred)/255 > 0.5\n",
    "    mask = mask.flatten()\n",
    "    pred = pred.flatten()\n",
    "    true_values.extend(mask.flatten())\n",
    "    predicted_values.extend(pred.flatten())\n",
    "    # precision += precision_score(mask, pred)\n",
    "    # recall += recall_score(mask, pred)\n",
    "    # f1 += f1_score(mask, pred)\n",
    "    # jaccard_indices = jaccard_score(mask, pred, average=None)\n",
    "    # iou += np.mean(jaccard_indices)\n",
    "    # specificity += specificity_score(mask, pred)\n",
    "    \n",
    "# # Convert lists to NumPy arrays\n",
    "# print(np.unique(true_values))\n",
    "# print(np.unique(predicted_values))\n",
    "true_values = np.array(true_values)/255 > 0.5\n",
    "predicted_values = np.array(predicted_values)/255 > 0.5\n",
    "# print(np.unique(true_values))\n",
    "# print(np.unique(predicted_values))\n",
    "# # Calculate metrics\n",
    "precision = precision_score(true_values, predicted_values)\n",
    "recall = recall_score(true_values, predicted_values)\n",
    "f1 = f1_score(true_values, predicted_values)\n",
    "# conf_matrix = confusion_matrix(true_values, predicted_values)\n",
    "\n",
    "# # Calculate mIoU (Jaccard Index)\n",
    "jaccard_indices = jaccard_score(true_values, predicted_values, average=None)\n",
    "miou = np.mean(jaccard_indices)\n",
    "\n",
    "# # Calculate mDice (Dice Coefficient)\n",
    "def dice_coef(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    total = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2.0 * intersection) / (total + 1e-6)\n",
    "\n",
    "mdice = dice_coef(true_values, predicted_values)\n",
    "\n",
    "print(\"mDice:\", np.mean(f1))\n",
    "print(\"mIoU:\",miou)\n",
    "print(\"Recall:\", np.mean(recall))\n",
    "print(\"Precision:\", np.mean(precision))\n",
    "\n",
    "# # mspecificity = specificity_score(true_values, predicted_values)\n",
    "# print(\"mDice:\", np.mean(f1)/len(mask_files))\n",
    "# print(\"mIoU:\", np.mean(iou)/len(mask_files))\n",
    "# print(\"Recall:\", np.mean(recall)/len(mask_files))\n",
    "# print(\"Precision:\", np.mean(precision)/len(mask_files))\n",
    "# # print(\"mSpecificity:\", np.mean(specificity)/len(mask_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fde4b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBefore sparse:\\nESFPNet [42] 0.8633 0.8291 0.8479 0.8794\\nOurs 0.8654 0.8332 0.8262 0.9084\\n\\nModel sparsity at last: 0.12221343932085313\\nmDice: 0.740787410554658\\nmIoU: 0.7173545153715384\\nRecall: 0.8073570862473162\\nPrecision: 0.7550762918005283\\n\\n\\nmDice: 0.8443018897407832\\nmIoU: 0.8181591820460904\\nRecall: 0.8579098010430998\\nPrecision: 0.8559038959010904\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UTTQ:\n",
    "'''\n",
    "Before sparse:\n",
    "ESFPNet [42] 0.8633 0.8291 0.8479 0.8794\n",
    "Ours 0.8654 0.8332 0.8262 0.9084\n",
    "\n",
    "Model sparsity at last: 0.12221343932085313\n",
    "mDice: 0.740787410554658\n",
    "mIoU: 0.7173545153715384\n",
    "Recall: 0.8073570862473162\n",
    "Precision: 0.7550762918005283\n",
    "\n",
    "\n",
    "mDice: 0.8443018897407832\n",
    "mIoU: 0.8181591820460904\n",
    "Recall: 0.8579098010430998\n",
    "Precision: 0.8559038959010904\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be14a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mDice: 0.8670030036500223\n",
    "mIoU: 0.8273643762673336\n",
    "Recall: 0.9062145795935481\n",
    "Precision: 0.8622526422860325\n",
    "mSpecificity: 0.9155333735236033\n",
    "# Model sparsity at last: 0.12221343932085313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88927964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# class LP(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Linear Prediction\n",
    "#     \"\"\"\n",
    "#     def __init__(self, input_dim=2048, embed_dim=768):\n",
    "#         super().__init__()\n",
    "#         self.proj = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B,_,H,W = x.shape\n",
    "#         print(\"Initial: \", x.shape)\n",
    "#         x = x.flatten(2)\n",
    "#         print(\"After flatten: \", x.shape)\n",
    "#         x = x.transpose(1, 2)\n",
    "#         print(\"After transpose: \", x.shape)\n",
    "#         x = self.proj(x)\n",
    "#         print(\"After linear: \", x.shape)\n",
    "#         return x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "# # Tạo một mô hình LP với input_dim=2048 và embed_dim=768\n",
    "# lp_model = LP(input_dim=2048, embed_dim=768)\n",
    "\n",
    "# # Tạo một ví dụ đầu vào x với kích thước (B, C, H, W) (batch size, channels, height, width)\n",
    "# input_x = torch.randn(8, 2048, 8, 8)  # Ví dụ với batch size là 8\n",
    "\n",
    "# # Sử dụng mô hình LP để thực hiện dự đoán tuyến tính\n",
    "# output = lp_model(input_x)\n",
    "\n",
    "# # Kết quả là một tensor có kích thước (B, H, W, embed_dim)\n",
    "# print(output.shape)  # In kích thước của đầu ra\n",
    "# # embed_dims=[64, 128, 256, 512]\n",
    "# tmp = nn.Conv2d(768, 1, kernel_size=1)(output)\n",
    "# print(tmp.shape)\n",
    "# # Lưu ý rằng đầu ra của mô hình LP có kích thước B x H x W x embed_dim,\n",
    "# # trong đó B là batch size, H và W là chiều cao và chiều rộng của đầu vào,\n",
    "# # và embed_dim là kích thước của embedding sau khi thực hiện linear projection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57c5ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch\n",
    "# # Define a class for your model\n",
    "# class LinearPredictionModel(nn.Module):\n",
    "#     def __init__(self, embed_dims):\n",
    "#         super(LinearPredictionModel, self).__init__()\n",
    "        \n",
    "#         # Calculate the input channels by summing up the embed_dims\n",
    "#         input_channels = sum(embed_dims)\n",
    "        \n",
    "#         # Create a 1x1 Conv2d layer for linear prediction\n",
    "#         self.linear_pred = nn.Conv2d(input_channels, 1, kernel_size=1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # Forward pass through the linear prediction layer\n",
    "#         out = self.linear_pred(x)\n",
    "#         return out\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming embed_dims is a list of embed dimensions [dim1, dim2, dim3, dim4]\n",
    "# embed_dims = [64, 128, 256, 512]\n",
    "\n",
    "# # Create an instance of the LinearPredictionModel\n",
    "# model = LinearPredictionModel(embed_dims)\n",
    "\n",
    "# # You can then use this model to perform linear predictions\n",
    "# # For example, you can pass some input tensor x through the model\n",
    "# # to get the predicted output:\n",
    "# input_tensor = torch.randn(1, sum(embed_dims), 10, 10)  # Example input tensor\n",
    "# print(input_tensor.shape)\n",
    "# output = model(input_tensor)\n",
    "# print(output.shape)\n",
    "# # 'output' will contain the predicted output of the linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e667a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
